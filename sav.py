# -*- coding: utf-8 -*-
"""SAV

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13L9FuUh_v7gl_dyQqIwVpp_EdJCH8SYz
"""

import streamlit as st
import google.generativeai as genai

# Load API key securely from a file
file_path = '/content/api.txt'
try:
    with open(file_path, mode='r') as key:
        api = key.read().strip()
    genai.configure(api_key=api)
except FileNotFoundError:
    st.error("‚ö†Ô∏è API key file not found! Please upload 'api.txt'.")
    st.stop()

# Initialize the Gemini AI model
model = genai.GenerativeModel(model_name="gemini-1.5-pro-latest",
                              system_instruction="You are an AI tutor that provides data science-related content.")

st.title("üí° AI Tutor - Data Science Assistant")
st.chat_message("assistant").write("Good Day! Welcome to AI Tutor. Feel free to ask your doubts!")

# Initialize session state for chat memory
if 'memory' not in st.session_state:
    st.session_state['memory'] = []

chat = model.start_chat(history=st.session_state['memory'])

# Display chat history
for msg in st.session_state['memory']:
    role = "assistant" if msg["role"] == "model" else "user"
    st.chat_message(role).write(msg["content"])

# User input
user_input = st.chat_input("Type your question here...")

if user_input:
    # Store user input
    st.session_state['memory'].append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    # Get AI response
    chat_response = chat.send_message(user_input)
    response_text = chat_response.text

    # Store AI response in memory
    st.session_state['memory'].append({"role": "model", "content": response_text})
    st.chat_message("assistant").write(response_text)

pip install streamlit